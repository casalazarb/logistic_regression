{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrSyU9UkOgGBr+obykBEoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casalazarb/logistic_regression/blob/main/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "K8phF73unm3o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#to load csv file to google colab\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "624WL4XdFexj",
        "outputId": "0ba75cbc-f503-4304-f070-c7b3a180dc17"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/loan.csv')"
      ],
      "metadata": {
        "id": "OS4q2LvvP3rD"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emRleiiWQMIG",
        "outputId": "c72172b9-8cbf-4082-80ef-2575f2eb8c37"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    default    rate  log_income  fico\n",
            "0         1  0.1496   10.714418   667\n",
            "1         1  0.1114   11.002100   722\n",
            "2         1  0.1343   11.884489   682\n",
            "3         1  0.1059   10.433822   687\n",
            "4         1  0.1501   12.269047   677\n",
            "5         1  0.0964   11.225243   772\n",
            "6         1  0.1280   10.373491   682\n",
            "7         1  0.1122   10.596635   712\n",
            "8         1  0.0964   11.472103   737\n",
            "9         1  0.1154    9.615805   672\n",
            "10        1  0.1501   11.472103   702\n",
            "11        1  0.1217   11.156251   672\n",
            "12        1  0.1375   11.038110   677\n",
            "13        1  0.1249   10.571317   667\n",
            "14        1  0.1028   10.727663   687\n",
            "15        0  0.1189   11.350407   737\n",
            "16        0  0.1071   11.082143   707\n",
            "17        0  0.1357   10.373491   682\n",
            "18        0  0.1008   11.350407   712\n",
            "19        0  0.1426   11.299732   667\n",
            "20        0  0.0788   11.904968   727\n",
            "21        0  0.1134   11.407565   682\n",
            "22        0  0.1221   10.203592   707\n",
            "23        0  0.1347   10.434116   677\n",
            "24        0  0.1324   11.835009   662\n",
            "25        0  0.0859   10.933107   767\n",
            "26        0  0.0714   11.512925   747\n",
            "27        0  0.0863    9.487972   727\n",
            "28        0  0.1103   10.738915   702\n",
            "29        0  0.1317   10.522773   672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([df['rate'], df['log_income'], df['fico']])\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057OgKH0QkTv",
        "outputId": "0ce555a1-7bd4-478a-f522-dfc2ffffa3ee"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = np.array([df['default']])\n",
        "print(Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzTk7KsaQlFw",
        "outputId": "ff4a3bec-2325-43f7-9154-e9a26f5d423f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid function\n",
        "\n",
        "def sigmoid(x_dot_thetas):\n",
        "    \"\"\"\n",
        "    Sigmoid of x_dot_thetas\n",
        "\n",
        "    Arguments:\n",
        "    x_dot_thetas: dot product of the value of the variables of each example with the parameters\n",
        "\n",
        "    Return:\n",
        "    sigmoid(x_dot_thetas)\n",
        "    \"\"\"\n",
        "\n",
        "    sigmoid = 1/(1+np.exp(-x_dot_thetas))\n",
        "   \n",
        "    return sigmoid"
      ],
      "metadata": {
        "id": "WXKoO5uRokEY"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_with_zeroes(n_param):\n",
        "    \"\"\"\n",
        "    Intialize the parameters with zeros\n",
        "\n",
        "    Argument:\n",
        "    n_param: number of parameters minus the constant\n",
        "    \n",
        "    Returns:\n",
        "    thetas: initialized vector with size number of rows and one column\n",
        "    theta_0: constant term\n",
        "    \"\"\"\n",
        "    \n",
        "    thetas = np.zeros((n_param, 1))\n",
        "    theta_0 = 0\n",
        "\n",
        "    return thetas, theta_0\n"
      ],
      "metadata": {
        "id": "_jQkofQVpLt7"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(thetas, theta_0, X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    thetas: parameters estimated in the regression\n",
        "    theta_0: constant term of the regression\n",
        "    X: matrix with observations to train the algorithm size of the matriz (n, number of parameters)\n",
        "    Y: label vector for a binary classification\n",
        "\n",
        "    Return:\n",
        "    cost: value of the cost function given the parameters and the obervations o examples, it has to be minimized\n",
        "    d_thetas: derivative of J (cost function) with respect to thetas, fundamental part in gradient descent\n",
        "    d_theta_0: derivative of J with respect to theta_0\n",
        "  \n",
        "    \"\"\"\n",
        "    \n",
        "    #n number of observations in matrix X\n",
        "    n = X.shape[1]\n",
        "    \n",
        "    #p is the probability that an example belongs to category or label 1\n",
        "    p = sigmoid(np.dot(np.transpose(thetas), X) + theta_0)\n",
        "    cost = -1/n * np.sum(Y * np.log(p) + (1-Y) * np.log(1-p))\n",
        "    \n",
        "    #derivatives of the cost function with respect to the parameters\n",
        "    d_thetas = 1/n * np.dot(X,np.transpose(p-Y))\n",
        "    d_theta_0 = 1/n * np.sum(p-Y)\n",
        "\n",
        "    cost = np.squeeze(cost)\n",
        "        \n",
        "    grads = {\"d_thetas\": d_thetas,\n",
        "             \"d_theta_0\": d_theta_0}\n",
        "    \n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "mClBo_yaBby2"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def optimize(thetas, theta_0, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    thetas: parameters estimated in the regression\n",
        "    theta_0: constant term of the regression\n",
        "    X: matrix with observations to train the algorithm size of the matriz (n, number of parameters)\n",
        "    Y: label vector for a binary classification\n",
        "    num_iterations: iterations for the optimization loop\n",
        "    learning_rate: learning rate of the gradient descent update rule\n",
        "    print_cost: True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params: dictionary containing the parameters thetas (size number of explicative variables) and constant term theta_0\n",
        "    grads: dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs: list of the costs computed during the optimization, hopefully it must decrease consistently\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "  \n",
        "    for iteration in range(num_iterations):        \n",
        "        \n",
        "        grads, cost = propagate(thetas, theta_0, X, Y)\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        d_thetas = grads[\"d_thetas\"]\n",
        "        d_theta_0 = grads[\"d_theta_0\"]\n",
        "        #update gradient descent\n",
        "        thetas -= learning_rate * d_thetas\n",
        "        theta_0 -= learning_rate * d_theta_0\n",
        "      \n",
        "        # Record the costs\n",
        "        if iteration % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and iteration % 100 == 0:\n",
        "            print (\"Cost after iteration {}: {}\".format(iteration, cost))\n",
        "    \n",
        "    params = {\"thetas\": thetas,\n",
        "              \"theta_0\": theta_0}\n",
        "    \n",
        "    grads = {\"d_thetas\": d_thetas,\n",
        "             \"d_theta_0\": d_theta_0}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "P3qcoD5xr2f8"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(thetas, theta_0, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    thetas: array of parameters\n",
        "    theta_0: the constant of the regression\n",
        "    X: data matrix with explicative variables\n",
        "    \n",
        "    Returns:\n",
        "    Y_pred: prediction \n",
        "    '''\n",
        "    \n",
        "    n = X.shape[1]\n",
        "    Y_pred = np.zeros((1,n))\n",
        "    thetas = thetas.reshape(X.shape[0], 1)\n",
        "    \n",
        "    #compute the probability of an example to belong to the category 1\n",
        "    p = sigmoid(np.dot(np.transpose(thetas), X) + theta_0)\n",
        "  \n",
        "    \n",
        "    for index in range(p.shape[1]):\n",
        "        \n",
        "        #give the predicted category 0 or 1 depending on the probability and a threshold\n",
        "        if p[0, index] > 0.5:\n",
        "            Y_pred[0, index] = 1\n",
        "        else:\n",
        "            Y_pred[0, index] = 0\n",
        "    \n",
        "    return p, Y_pred"
      ],
      "metadata": {
        "id": "vvqU-DkL3dJg"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Integrates all the parts of the logistic regression model\n",
        "    \n",
        "    Arguments:\n",
        "    X_train: training set of the explicative variables size or the array (number of parameters - 1, n)\n",
        "    Y_train: training set of the  labels dimension n\n",
        "    X_test: testing set X\n",
        "    Y_test: testing set Y\n",
        "    num_iterations: for the optimization algorithm\n",
        "    learning_rate: to update gradient descent\n",
        "    print_cost: information for the user behavior of the cost function, hopefully decreasing\n",
        "    \n",
        "    Returns:\n",
        "    output: dictionary containing information about the model, parameters, predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros\n",
        "    thetas, theta_0 = initialize_with_zeroes(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(thetas, theta_0, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    #store the parameters of the model\n",
        "    thetas = parameters[\"thetas\"]\n",
        "    theta_0 = parameters[\"theta_0\"]\n",
        "    \n",
        "    #Predict group of test/train \n",
        "    Y_predgroup_train = predict(thetas, theta_0, X_train)[1]\n",
        "    Y_predgroup_test = predict(thetas, theta_0, X_test)[1]\n",
        "\n",
        "\n",
        "    #Predict probability of belonging to group 1 for test/train\n",
        "    Y_predprob_train = predict(thetas, theta_0, X_train)[0]\n",
        "    Y_predprob_test = predict(thetas, theta_0, X_test)[0]\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format((1 - np.mean(np.abs(Y_predgroup_train - Y_train))) * 100))\n",
        "    print(\"test accuracy: {} %\".format((1 - np.mean(np.abs(Y_predgroup_test - Y_test))) * 100))\n",
        "\n",
        "    \n",
        "    output = {\"costs\": costs,\n",
        "              \"thetas\" : thetas, \n",
        "              \"theta_0\" : theta_0,\n",
        "              \"learning_rate\" : learning_rate,\n",
        "              \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "W52H7qrS9_qp"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = logistic_regression(X_train, Y_train, X_train, Y_train, num_iterations = 10, learning_rate = 0.07)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "63PIvjn-Tasi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}